'''
import pandas as pd
from sklearn.preprocessing import LabelEncoder

df = pd.read_csv('datasets/petfinder-mini/petfinder-mini.csv')

col = ['Type', 'Breed1', 'Gender', 'Color1', 'Color2', 'MaturitySize','FurLength', 'Vaccinated', 'Sterilized', 'Health']

df.drop(['Description'],axis=1,inplace=True)
df[col] = df[col].astype(str).apply(LabelEncoder().fit_transform)
#df.to_csv('datasets/petfinder-mini/petfinder-mini.csv')
df = df.astype('int32')
df.to_parquet('datasets/petfinder_mini.parquet',index=False)
'''
import os
from petastorm import make_batch_reader
from petastorm.tf_utils import make_petastorm_dataset
import tensorflow as tf
from tensorflow.data.experimental import unbatch
from tensorflow.io import decode_raw
from tensorflow.keras.applications.xception import preprocess_input
from tensorflow.keras.layers import Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
# from pyspark.context import SparkContext
# from pyspark.sql.session import SparkSession
#sc = SparkContext('local')
# spark = SparkSession(sc)
from tensorflow.keras import layers



from adatasets import test_dataset_classification_fake
df, d = test_dataset_classification_fake(nrows=100)
print(df)
colnum, colcat, coly = d['colnum'], d['colcat'], d['coly']

path = os.path.abspath("data/input/ztest/fake/").replace("\\","/")
os.makedirs(path, exist_ok=True)

df.to_parquet(path + "/feature_01.parquet")
df.to_parquet(path + "/feature_02.parquet")



def pack_features_vector(features, labels):
    """Pack the features into a single array."""
    features = tf.stack(list(features.values()), axis=1)
    return features, labels

path2 = 'file:' + path +"/feature_01.parquet" #.replace("D:/", "")


batch_size = 32
with make_batch_reader( path2 ) as reader:
    dataset  = make_petastorm_dataset(reader)
    iterator = dataset.make_one_shot_iterator()

    tensor = iterator.get_next()

    print("dataset", dataset, )


    model = tf.keras.Sequential([
           layers.Flatten(),
           layers.Dense(256, activation='elu'),
           layers.Dense(32,  activation='elu'),
           layers.Dense(1,   activation='sigmoid')
           ])
    model.compile(optimizer='adam',
                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
                  metrics=['accuracy'])

    model.fit([tensor],
           steps_per_epoch=1,
           epochs=1,
           verbose=1
           )
    print('Hurray Successfully Initiated')




########################################################################################################################
"""
https://github.com/uber/petastorm/tree/master/examples/hello_world/external_dataset

"""
from petastorm.tf_utils import tf_tensors, make_petastorm_dataset
def tensorflow_hello_world(dataset_url='file:///tmp/external_dataset'):
    # Example: tf_tensors will return tensors with dataset data
    with make_batch_reader(dataset_url) as reader:
        tensor = tf_tensors(reader)
        with tf.Session() as sess:
            # Because we are using make_batch_reader(), each read returns a batch of rows instead of a single row
            batched_sample = sess.run(tensor)
            print("id batch: {0}".format(batched_sample.id))

    # Example: use tf.data.Dataset API
    with make_batch_reader(dataset_url) as reader:
        dataset = make_petastorm_dataset(reader)
        iterator = dataset.make_one_shot_iterator()
        tensor = iterator.get_next()
        with tf.Session() as sess:
            batched_sample = sess.run(tensor)
            print("id batch: {0}".format(batched_sample.id))





"""Minimal example of how to read samples from a dataset generated by `generate_external_dataset.py`
using pytorch, using make_batch_reader() instead of make_reader()"""
from petastorm import make_batch_reader
from petastorm.pytorch import DataLoader


def pytorch_hello_world(dataset_url='file:///tmp/external_dataset'):
    with DataLoader(make_batch_reader(dataset_url)) as train_loader:
        sample = next(iter(train_loader))
        # Because we are using make_batch_reader(), each read returns a batch of rows instead of a single row
        print("id batch: {0}".format(sample['id']))



"""Minimal example of how to read samples from a dataset generated by `generate_non_petastorm_dataset.py`
using plain Python"""

from petastorm import make_batch_reader


def python_hello_world(dataset_url='file:///tmp/external_dataset'):
    # Reading data from the non-Petastorm Parquet via pure Python
    with make_batch_reader(dataset_url, schema_fields=["id", "value1", "value2"]) as reader:
        for schema_view in reader:
            # make_batch_reader() returns batches of rows instead of individual rows
            print("Batched read:\nid: {0} value1: {1} value2: {2}".format(
                schema_view.id, schema_view.value1, schema_view.value2))













